{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bc7fbb",
   "metadata": {},
   "source": [
    "# ðŸ—ï¸ Full Pipeline Walkthrough: Multi-Document Extraction\n",
    "\n",
    "\n",
    "**What you'll learn:**\n",
    "- Complete custom pipeline with Neo4j GraphRAG\n",
    "- Page-by-page PDF processing with Gemini vision\n",
    "- Robust error handling (JSON repair, timeouts, failures)\n",
    "- Basic entity resolution across multiple documents\n",
    "- Working with pre-loaded data\n",
    "\n",
    "**Important:** The extraction process takes 1+ hour for all documents. We'll walk through the code, then use pre-loaded data for queries (part 4)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š What This Pipeline Does\n",
    "\n",
    "This notebook shows a pipeline that:\n",
    "1. **Loads PDFs page-by-page** - Each page processed individually\n",
    "2. **Extracts entities with Gemini vision** - Sees tables, logos, visual layout\n",
    "3. **Creates lexical graph** - Documentâ†’Chunkâ†’Entity relationships\n",
    "4. **Handles errors gracefully** - JSON repair, timeouts, skip failures\n",
    "5. **Writes to Neo4j** - Complete knowledge graph with all connections\n",
    "\n",
    "**Documents processed:** 4 pharmaceutical pipeline reports (~150 pages total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1916d483",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Install Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install google-genai pymupdf python-dotenv \"neo4j-graphrag[experimental]\" json-repair neo4j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3081ac",
   "metadata": {},
   "source": [
    "## ðŸ” Step 2: Configure Credentials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a557a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Credentials configured\n",
      "  Connecting to: neo4j://127.0.0.1:7687\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Replace with provided credentials\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'neo4j+s://SHARED-INSTANCE.databases.neo4j.io')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME', 'neo4j')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD', 'shared-password-here')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')\n",
    "\n",
    "# Gemini API key (only needed if you want to run extraction)\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', 'your-key-here')\n",
    "\n",
    "print(\"âœ“ Credentials configured\")\n",
    "print(f\"  Connecting to: {NEO4J_URI.split('@')[-1] if '@' in NEO4J_URI else NEO4J_URI}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b0f49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ”§ Pipeline Components\n",
    "\n",
    "The following cells define the custom components for our pipeline.  \n",
    "**You can read through the code to understand how it works.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab74b3",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 3: Define Schema\n",
    "\n",
    "Same pharmaceutical schema as previous notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51f3aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Schema components defined\n"
     ]
    }
   ],
   "source": [
    "from neo4j_graphrag.experimental.components.schema import SchemaBuilder, NodeType, RelationshipType\n",
    "\n",
    "# Node types\n",
    "node_types = [\n",
    "    NodeType(label=\"Molecule\", description=\"Drug or therapeutic molecule\"),\n",
    "    NodeType(label=\"Company\", description=\"Pharmaceutical company\"),\n",
    "    NodeType(label=\"Target\", description=\"Biological target (protein, pathway, etc)\"),\n",
    "    NodeType(label=\"Disease\", description=\"Disease or medical condition\"),\n",
    "]\n",
    "\n",
    "# Relationship types\n",
    "relationship_types = [\n",
    "    RelationshipType(label=\"TREATS\", description=\"Molecule treats disease\"),\n",
    "    RelationshipType(label=\"TARGETS\", description=\"Molecule targets biological target\"),\n",
    "    RelationshipType(label=\"ASSOCIATED\", description=\"Disease associated with target\"),\n",
    "    RelationshipType(label=\"IN_PIPELINE\", description=\"Company has molecule in pipeline\"),\n",
    "]\n",
    "\n",
    "# Expected patterns\n",
    "patterns = [\n",
    "    (\"Molecule\", \"TREATS\", \"Disease\"),\n",
    "    (\"Company\", \"IN_PIPELINE\", \"Molecule\"),\n",
    "    (\"Molecule\", \"TARGETS\", \"Target\"),\n",
    "    (\"Disease\", \"ASSOCIATED\", \"Target\"),\n",
    "]\n",
    "\n",
    "print(\"âœ“ Schema components defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f2568",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 4: Page-by-Page PDF Loader\n",
    "\n",
    "Custom component that loads PDFs and processes each page individually.\n",
    "\n",
    "**Why page-by-page?**\n",
    "- Better extraction quality (focused context per page)\n",
    "- More granular error handling\n",
    "- Easier to track progress\n",
    "- Can parallelize in production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbdf8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PageByPagePdfLoader defined\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from neo4j_graphrag.experimental.components.pdf_loader import PdfLoader\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks, DocumentInfo\n",
    "from neo4j_graphrag.experimental.pipeline.component import DataModel\n",
    "from pydantic import validate_call\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "class PageByPageResult(DataModel):\n",
    "    \"\"\"Result containing chunks and document info.\"\"\"\n",
    "    chunks: TextChunks\n",
    "    document_info: DocumentInfo\n",
    "\n",
    "class PageByPagePdfLoader(PdfLoader):\n",
    "    \"\"\"Custom PDF loader that processes each page separately.\"\"\"\n",
    "    \n",
    "    @validate_call\n",
    "    async def run(self, filepath: Union[str, Path]) -> PageByPageResult:\n",
    "        filepath = str(filepath)\n",
    "        doc = fitz.open(filepath)\n",
    "        chunks = []\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()  # Plain text\n",
    "            \n",
    "            # Extract single page as PDF bytes\n",
    "            page_pdf = fitz.open()\n",
    "            page_pdf.insert_pdf(doc, from_page=page_num, to_page=page_num)\n",
    "            pdf_bytes = page_pdf.tobytes()\n",
    "            page_pdf.close()\n",
    "            \n",
    "            # Create chunk with both text and PDF bytes\n",
    "            chunks.append(TextChunk(\n",
    "                text=text,\n",
    "                index=page_num,\n",
    "                metadata={\n",
    "                    'pdf_bytes': pdf_bytes,  # For Gemini vision\n",
    "                    'page_number': page_num + 1,\n",
    "                    'source_file': os.path.basename(filepath)\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        document_info = DocumentInfo(\n",
    "            path=filepath,\n",
    "            document_type=\"pdf\"\n",
    "        )\n",
    "        \n",
    "        return PageByPageResult(\n",
    "            chunks=TextChunks(chunks=chunks),\n",
    "            document_info=document_info\n",
    "        )\n",
    "\n",
    "print(\"âœ“ PageByPagePdfLoader defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff658b5",
   "metadata": {},
   "source": [
    "## ðŸ›¡ï¸ Step 5: Robust Gemini PDF Extractor\n",
    "\n",
    "This extractor includes 3 fixes:\n",
    "\n",
    "1. **JSON Repair** - Fixes malformed JSON from Gemini (sometimes happens with complex outputs)\n",
    "2. **Timeouts** - Prevents hanging on slow API calls (60s limit per page)\n",
    "3. **OnError.IGNORE** - Continues processing even if individual pages fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f54eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GeminiPdfExtractor defined\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "import json\n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from json_repair import repair_json\n",
    "from neo4j_graphrag.experimental.components.entity_relation_extractor import EntityRelationExtractor, OnError\n",
    "from neo4j_graphrag.experimental.components.types import Neo4jGraph, Neo4jNode, Neo4jRelationship, LexicalGraphConfig\n",
    "from neo4j_graphrag.experimental.components.schema import GraphSchema\n",
    "from neo4j_graphrag.experimental.components.lexical_graph import LexicalGraphBuilder\n",
    "\n",
    "class GeminiPdfExtractor(EntityRelationExtractor):\n",
    "    \"\"\"Robust Gemini extractor with error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, gemini_client, create_lexical_graph: bool = True, on_error: OnError = OnError.IGNORE):\n",
    "        super().__init__(create_lexical_graph=create_lexical_graph, on_error=on_error)\n",
    "        self.client = gemini_client\n",
    "    \n",
    "    def _build_schema_text(self, schema: GraphSchema) -> str:\n",
    "        \"\"\"Convert schema to prompt text.\"\"\"\n",
    "        node_labels = [node.label for node in schema.node_types]\n",
    "        rel_types = [rel.label for rel in schema.relationship_types]\n",
    "        \n",
    "        schema_lines = []\n",
    "        schema_lines.append(\"Node Labels: \" + \", \".join(node_labels))\n",
    "        schema_lines.append(\"Relationship Types: \" + \", \".join(rel_types))\n",
    "        \n",
    "        if schema.patterns:\n",
    "            schema_lines.append(\"Patterns:\")\n",
    "            for start, rel, end in schema.patterns:\n",
    "                schema_lines.append(f\"- ({start})-[{rel}]->({end})\")\n",
    "        \n",
    "        return \"\\n\".join(schema_lines)\n",
    "    \n",
    "    async def extract_for_chunk(self, schema: GraphSchema, chunk: TextChunk, pdf_bytes: bytes) -> Neo4jGraph:\n",
    "        \"\"\"Extract entities from a single page with error handling.\"\"\"\n",
    "        schema_text = self._build_schema_text(schema) if schema else \"\"\n",
    "        \n",
    "        prompt_template = f\"\"\"You are a medical researcher extracting information from pharmaceutical documents.\n",
    "\n",
    "Extract entities and relationships from this PDF page.\n",
    "\n",
    "Return JSON:\n",
    "{{\"nodes\": [{{\"id\": \"0\", \"label\": \"entity_type\", \"properties\": {{\"name\": \"entity_name\"}}}}],\n",
    "  \"relationships\": [{{\"type\": \"REL_TYPE\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {{\"details\": \"description\"}}}}]}}\n",
    "\n",
    "Schema:\n",
    "{schema_text}\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # FIX 2: Add timeout (60 seconds)\n",
    "            async def call_gemini():\n",
    "                return self.client.models.generate_content(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    contents=[\n",
    "                        types.Part.from_bytes(data=pdf_bytes, mime_type='application/pdf'),\n",
    "                        prompt_template\n",
    "                    ],\n",
    "                    config=types.GenerateContentConfig(response_mime_type=\"application/json\")\n",
    "                )\n",
    "            \n",
    "            response = await asyncio.wait_for(call_gemini(), timeout=60.0)\n",
    "            \n",
    "            # FIX 1: JSON repair for malformed JSON\n",
    "            try:\n",
    "                graph_data = json.loads(response.text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"    âš  JSON error, attempting repair...\")\n",
    "                repaired_json = repair_json(response.text)\n",
    "                graph_data = json.loads(repaired_json)\n",
    "                print(f\"    âœ“ JSON repaired\")\n",
    "            \n",
    "            # Convert to Neo4j graph objects\n",
    "            nodes = [\n",
    "                Neo4jNode(\n",
    "                    id=node['id'],\n",
    "                    label=node['label'],\n",
    "                    properties=node.get('properties', {})\n",
    "                )\n",
    "                for node in graph_data.get('nodes', [])\n",
    "            ]\n",
    "            \n",
    "            relationships = [\n",
    "                Neo4jRelationship(\n",
    "                    type=rel['type'],\n",
    "                    start_node_id=rel['start_node_id'],\n",
    "                    end_node_id=rel['end_node_id'],\n",
    "                    properties=rel.get('properties', {})\n",
    "                )\n",
    "                for rel in graph_data.get('relationships', [])\n",
    "            ]\n",
    "            \n",
    "            return Neo4jGraph(nodes=nodes, relationships=relationships)\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"    âœ— Timeout - skipping page\")\n",
    "            return Neo4jGraph(nodes=[], relationships=[])\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— Error: {type(e).__name__} - skipping page\")\n",
    "            return Neo4jGraph(nodes=[], relationships=[])\n",
    "    \n",
    "    @validate_call\n",
    "    async def run(\n",
    "        self, \n",
    "        chunks: TextChunks, \n",
    "        document_info: Optional[DocumentInfo] = None,\n",
    "        lexical_graph_config: Optional[LexicalGraphConfig] = None,\n",
    "        schema: Optional[GraphSchema] = None, \n",
    "        **kwargs: Any\n",
    "    ) -> Neo4jGraph:\n",
    "        \"\"\"Process all chunks.\"\"\"\n",
    "        # Create lexical graph (Documentâ†’Chunk relationships)\n",
    "        lexical_graph_builder = None\n",
    "        lexical_graph = None\n",
    "        \n",
    "        if self.create_lexical_graph:\n",
    "            config = lexical_graph_config or LexicalGraphConfig()\n",
    "            lexical_graph_builder = LexicalGraphBuilder(config=config)\n",
    "            lexical_graph_result = await lexical_graph_builder.run(\n",
    "                text_chunks=chunks, \n",
    "                document_info=document_info\n",
    "            )\n",
    "            lexical_graph = lexical_graph_result.graph\n",
    "        \n",
    "        schema = schema or GraphSchema(node_types=())\n",
    "        chunk_graphs = []\n",
    "        \n",
    "        # Process each page\n",
    "        for chunk in chunks.chunks:\n",
    "            pdf_bytes = chunk.metadata.pop('pdf_bytes')\n",
    "            page_num = chunk.metadata.get('page_number', '?')\n",
    "            \n",
    "            print(f\"  Processing page {page_num}...\")\n",
    "            \n",
    "            try:\n",
    "                chunk_graph = await self.extract_for_chunk(schema, chunk, pdf_bytes)\n",
    "                self.update_ids(chunk_graph, chunk)\n",
    "                \n",
    "                if lexical_graph_builder:\n",
    "                    await lexical_graph_builder.process_chunk_extracted_entities(\n",
    "                        chunk_graph, chunk\n",
    "                    )\n",
    "                \n",
    "                chunk_graphs.append(chunk_graph)\n",
    "            except Exception as e:\n",
    "                # FIX 3: Continue on error\n",
    "                print(f\"    âœ— Failed, continuing...\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all graphs\n",
    "        if lexical_graph:\n",
    "            graph = lexical_graph.model_copy(deep=True)\n",
    "        else:\n",
    "            graph = Neo4jGraph()\n",
    "        \n",
    "        for chunk_graph in chunk_graphs:\n",
    "            graph.nodes.extend(chunk_graph.nodes)\n",
    "            graph.relationships.extend(chunk_graph.relationships)\n",
    "        \n",
    "        return graph\n",
    "\n",
    "print(\"âœ“ GeminiPdfExtractor defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08d6ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸš€ Pipeline Setup (Commented Out - 1+ Hour Runtime)\n",
    "\n",
    "The following cells show how to build and run the pipeline.  \n",
    "**These are commented out** since extraction takes over an hour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888ce24",
   "metadata": {},
   "source": [
    "## Step 6: Build Pipeline (Commented Out)\n",
    "\n",
    "This shows how to connect all components together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT - This code works but takes 1+ hour to run\n",
    "#\n",
    "# import neo4j\n",
    "# from neo4j_graphrag.experimental.pipeline import Pipeline\n",
    "# from neo4j_graphrag.experimental.components.kg_writer import Neo4jWriter\n",
    "#\n",
    "# # Initialize components\n",
    "# gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "# driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "#\n",
    "# pdf_loader = PageByPagePdfLoader()\n",
    "# schema_builder = SchemaBuilder()\n",
    "# extractor = GeminiPdfExtractor(gemini_client, create_lexical_graph=True)\n",
    "# writer = Neo4jWriter(driver=driver, neo4j_database=NEO4J_DATABASE)\n",
    "#\n",
    "# # Build pipeline\n",
    "# pipe = Pipeline()\n",
    "# pipe.add_component(pdf_loader, \"pdf_loader\")\n",
    "# pipe.add_component(schema_builder, \"schema\")\n",
    "# pipe.add_component(extractor, \"extractor\")\n",
    "# pipe.add_component(writer, \"writer\")\n",
    "#\n",
    "# # Connect components\n",
    "# pipe.connect(\"pdf_loader\", \"extractor\", {\"chunks\": \"pdf_loader.chunks\", \"document_info\": \"pdf_loader.document_info\"})\n",
    "# pipe.connect(\"schema\", \"extractor\", {\"schema\": \"schema\"})\n",
    "# pipe.connect(\"extractor\", \"writer\", {\"graph\": \"extractor\"})\n",
    "#\n",
    "# print(\"âœ“ Pipeline built\")\n",
    "\n",
    "print(\"âš ï¸  Pipeline code commented out (takes 1+ hour to run)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c2b85",
   "metadata": {},
   "source": [
    "## Step 7: Process Documents (Commented Out)\n",
    "\n",
    "This is how you would run the pipeline on all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT - This code works but takes 1+ hour\n",
    "#\n",
    "# documents = [\n",
    "#     \"/workshop-data/AbbVie Long-Term Guidance and Pipeline Update.pdf\",\n",
    "#     \"/workshop-data/BMY-2024-Q1-Results-Investor-Presentation-with-Appendix.pdf\",\n",
    "#     \"/workshop-data/JNJ-Pipeline-2Q2024.pdf\",\n",
    "#     \"/workshop-data/ph-rd-pipeline-2025-07-24-update-20250725.pdf\",\n",
    "# ]\n",
    "#\n",
    "# async def process_document(filepath):\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Processing: {os.path.basename(filepath)}\")\n",
    "#     print('='*60)\n",
    "#     \n",
    "#     pipe_inputs = {\n",
    "#         \"pdf_loader\": {\"filepath\": filepath},\n",
    "#         \"schema\": {\n",
    "#             \"node_types\": node_types,\n",
    "#             \"relationship_types\": relationship_types,\n",
    "#             \"patterns\": patterns,\n",
    "#         }\n",
    "#     }\n",
    "#     \n",
    "#     result = await pipe.run(pipe_inputs)\n",
    "#     print(f\"âœ“ Completed: {os.path.basename(filepath)}\")\n",
    "#     return result\n",
    "#\n",
    "# # Process all documents\n",
    "# import time\n",
    "# results = []\n",
    "# start_time = time.time()\n",
    "#\n",
    "# for doc_path in documents:\n",
    "#     if os.path.exists(doc_path):\n",
    "#         try:\n",
    "#             result = await process_document(doc_path)\n",
    "#             results.append(result)\n",
    "#         except Exception as e:\n",
    "#             print(f\"âœ— Error processing {doc_path}: {e}\")\n",
    "#\n",
    "# elapsed = time.time() - start_time\n",
    "# print(f\"\\nâœ… All documents processed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "print(\"âš ï¸  Extraction code commented out\")\n",
    "print(\"ðŸ’¡ Extraction would process ~150 pages across 4 documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19dc394",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ—„ï¸ Accessing Pre-Loaded Data\n",
    "\n",
    "Instead of running the 1+ hour extraction, we'll connect to a database with pre-loaded data.\n",
    "\n",
    "**Two options:**\n",
    "\n",
    "### Option 1: Shared Aura Instance (Recommended)\n",
    "- Instructor provides credentials\n",
    "- Read-only access\n",
    "- Instant access to full dataset\n",
    "\n",
    "### Option 2: Load from Dump File\n",
    "- Download Neo4j dump file from: `[LINK TO BE PROVIDED]`\n",
    "- Load into your own Neo4j instance\n",
    "- Full control for experimentation\n",
    "\n",
    "**For this workshop, use the shared Aura instance** (credentials in Step 2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42be918",
   "metadata": {},
   "source": [
    "## Step 8: Connect to Neo4j with Pre-Loaded Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae31ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Connected to Neo4j\n",
      "  URI: neo4j://127.0.0.1:7687\n",
      "  Database: neo4j\n"
     ]
    }
   ],
   "source": [
    "import neo4j\n",
    "\n",
    "# Connect to Neo4j (using credentials from Step 2)\n",
    "driver = neo4j.GraphDatabase.driver(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    driver.verify_connectivity()\n",
    "    print(f\"âœ“ Connected to Neo4j\")\n",
    "    print(f\"  URI: {NEO4J_URI}\")\n",
    "    print(f\"  Database: {NEO4J_DATABASE}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Connection failed: {e}\")\n",
    "    print(\"Check your credentials in Step 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c2abc",
   "metadata": {},
   "source": [
    "## Step 9: Explore the Graph\n",
    "\n",
    "Let's see what's in the pre-loaded database!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cc274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: HAS_CHUNK)} {position: line: 1, column: 13, offset: 12} for query: 'MATCH ()-[r:HAS_CHUNK]->() RETURN count(r) as count'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Graph Statistics:\n",
      "\n",
      "  Molecule: 360\n",
      "  Company: 61\n",
      "  Target: 164\n",
      "  Disease: 308\n",
      "  Document: 4\n",
      "  Chunk: 84\n",
      "\n",
      "Relationships:\n",
      "  TREATS: 499\n",
      "  TARGETS: 203\n",
      "  ASSOCIATED: 157\n",
      "  IN_PIPELINE: 335\n",
      "  NEXT_CHUNK: 80\n",
      "  FROM_CHUNK: 1,320\n",
      "\n",
      "âœ“ Rich knowledge graph with ~150 pages extracted!\n"
     ]
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    \"\"\"Helper to run Cypher queries.\"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        result = session.run(query)\n",
    "        return result.data()\n",
    "\n",
    "# Get graph statistics\n",
    "print(\"ðŸ“Š Graph Statistics:\\n\")\n",
    "\n",
    "# Nodes by type\n",
    "for label in [\"Molecule\", \"Company\", \"Target\", \"Disease\", \"Document\", \"Chunk\"]:\n",
    "    count = run_query(f\"MATCH (n:{label}) RETURN count(n) as count\")[0]['count']\n",
    "    print(f\"  {label}: {count:,}\")\n",
    "\n",
    "# Relationships by type\n",
    "print(\"\\nRelationships:\")\n",
    "for rel_type in [\"TREATS\", \"TARGETS\", \"ASSOCIATED\", \"IN_PIPELINE\", \"HAS_CHUNK\", \"NEXT_CHUNK\", \"FROM_CHUNK\"]:\n",
    "    count = run_query(f\"MATCH ()-[r:{rel_type}]->() RETURN count(r) as count\")[0]['count']\n",
    "    if count > 0:\n",
    "        print(f\"  {rel_type}: {count:,}\")\n",
    "\n",
    "print(\"\\nâœ“ Rich knowledge graph with ~150 pages extracted!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b5480",
   "metadata": {},
   "source": [
    "## Step 10: Sample Queries\n",
    "\n",
    "Let's run some interesting queries on the full dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c7a6e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¢ Top Companies by Pipeline Size:\n",
      "\n",
      "  Bristol Myers Squibb: 100 molecules\n",
      "  Bayer: 59 molecules\n",
      "  AbbVie: 59 molecules\n",
      "  J&J: 53 molecules\n",
      "  Calibr: 2 molecules\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Top companies by pipeline size\n",
    "print(\"ðŸ¢ Top Companies by Pipeline Size:\\n\")\n",
    "query = \"\"\"\n",
    "MATCH (c:Company)-[:IN_PIPELINE]->(m:Molecule)\n",
    "RETURN c.name as company, count(m) as molecules\n",
    "ORDER BY molecules DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "results = run_query(query)\n",
    "for r in results:\n",
    "    print(f\"  {r['company']}: {r['molecules']} molecules\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7081109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Most Targeted Diseases:\n",
      "\n",
      "  Solid Tumors: 19 molecules\n",
      "  Advanced Solid Tumors: 16 molecules\n",
      "  NSCLC: 11 molecules\n",
      "  Oncology: 10 molecules\n",
      "  Hematology: 10 molecules\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Most targeted diseases\n",
    "print(\"ðŸŽ¯ Most Targeted Diseases:\\n\")\n",
    "query = \"\"\"\n",
    "MATCH (m:Molecule)-[:TREATS]->(d:Disease)\n",
    "RETURN d.name as disease, count(m) as molecules\n",
    "ORDER BY molecules DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "results = run_query(query)\n",
    "for r in results:\n",
    "    print(f\"  {r['disease']}: {r['molecules']} molecules\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f2728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Š Sample Molecules with Context:\n",
      "\n",
      "  Skyrizi (AbbVie) â†’ treats Psoriasis/Psoriatic Arthritis\n",
      "  Skyrizi (AbbVie) â†’ treats Inflammatory Bowel Disease\n",
      "  Skyrizi (AbbVie) â†’ treats Psoriatic Arthritis\n",
      "  Skyrizi (AbbVie) â†’ treats Crohn's Disease\n",
      "  Skyrizi (AbbVie) â†’ treats Psoriasis\n",
      "  Skyrizi (AbbVie) â†’ treats UC\n",
      "  Rinvoq (AbbVie) â†’ treats Inflammatory Bowel Disease\n",
      "  Rinvoq (AbbVie) â†’ treats Rheumatology\n",
      "  Rinvoq (AbbVie) â†’ treats Dermatology\n",
      "  Rinvoq (AbbVie) â†’ treats Rheumatoid Arthritis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Molecules with their context (company and indication)\n",
    "print(\"ðŸ’Š Sample Molecules with Context:\\n\")\n",
    "query = \"\"\"\n",
    "MATCH (c:Company)-[:IN_PIPELINE]->(m:Molecule)-[:TREATS]->(d:Disease)\n",
    "RETURN m.name as molecule, c.name as company, d.name as disease\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "results = run_query(query)\n",
    "for r in results:\n",
    "    print(f\"  {r['molecule']} ({r['company']}) â†’ treats {r['disease']}\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644ba8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ What You've Learned\n",
    "\n",
    "Congratulations! You've seen:\n",
    "\n",
    "âœ… **Production-ready pipeline** with robust error handling  \n",
    "âœ… **Page-by-page processing** for better extraction quality  \n",
    "âœ… **Gemini vision** for understanding complex PDF layouts  \n",
    "âœ… **Lexical graph** connecting Documentsâ†’Chunksâ†’Entities  \n",
    "âœ… **Multi-document extraction** across 4 pharmaceutical reports  \n",
    "âœ… **Complete knowledge graph** ready for advanced queries  \n",
    "\n",
    "## ðŸ”‘ Key Takeaways\n",
    "\n",
    "1. **Custom components** give you full control over the pipeline\n",
    "2. **Error handling** (JSON repair, timeouts) is essential for production\n",
    "3. **Lexical graph** provides traceable entity-to-source connections\n",
    "4. **Page-by-page** processing enables better context and parallelization\n",
    "5. **Pre-loading data** speeds up development and workshops\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Next Steps\n",
    "\n",
    "In **Part 4**, you'll see:\n",
    "- Agentic GraphRAG patterns\n",
    "- Text2Cypher with MCP\n",
    "- Building conversational agents\n",
    "- Advanced query patterns\n",
    "\n",
    "**Next Notebook:** Agentic GraphRAG â†’\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8650af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3449fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close driver connection\n",
    "driver.close()\n",
    "print(\"âœ“ Connection closed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
